### Boundri 

## Project Summary 

Related Repos: 
    - https://github.com/MAP-Team/data-science
    - https://github.com/MAP-Team/haptic-camera-system
    - https://github.com/lukeaparker/NURV

Our team has been working on Boundri in the hopes of utilizing neural networks and deep learning to build an unobtrusive sensory enhancement system for the blind and visually impaired. The World Health Organization estimates that 285 million individuals experience blindness or visual impairment around the globe. Of that population, it is estimated that 2% to 8% of the population use white canes while 5% use guide-dogs. To us, this shows that there is a serious flaw in our modern mobility aids. The aim at first was to build a system to replace the white cane because of its many functional failures and increased insecurity (socially and physically). So we started off by using OpenCV and two cameras to generate disparity maps that we then used for calculating depth maps. The idea was to then use a haptic feedback wearable to map distances of obstacles to the specific region of the body that would be at risk of collision. After building out a prototype consisting of a Flask app that streamed left camera, right camera, disparity, and depth, we ran into numerous problems getting the system to run in real-time including implementing threading and a frame buffer. We concluded after some time that this was an incredibly sophisticated task to tackle as our first feature. So we took a step back. 

Thinking about the problem more simply, we thought about how we could make the world simpler to navigate with no vision. We thought, what if you could build a system that would be able to connect to a camera mounted on a user's upper chest to provide a nearly first-person perspective while retaining discrete unobtrusive sensor appearance and location. Now that the system has vision, what if it could integrate natively with a user’s smartphone assistant? This way users could perform visual context queries about what the camera could see from its perspective by just asking their voice assistant. 

This idea has two primary components. The first component is an API built in Go that will direct  routes  to deployed deep learning models. These models will be trained to identify basic attributes like text and objects. When a user submits a query, it will identify the context needed to fulfill the question, and then visit the routes which will return JSON of the prediction generated by the forward pass through the deployed model. The system can then use that context to formulate a response to the query. The second component is a camera system that connects to a user’s phone to take photos that will be sent to the API for analysis. Because this system is supposed to be discrete, we are designing a small camera that can be securely mounted to a user’s upper chest near the collarbone. While we are excited about the development of the sensors, we are not as focused on its development in the immediate future as we can use the onboard phone camera in lieu. 

## User Stories 

John is visually impaired and lives in San Francisco. He is walking to work and approaches a street corner. He looks around to see the lights but it’s too bright. He listens for blocker cars, but can’t hear any. He pulls out his phone and asks his voice assistant “(wake word), Boundri, check if it’s safe to cross.” The system takes a picture, converts it into a byte array, and submits a get request to the API query being “is it safe to cross”, along with the byte array. The API analyzes the query and the byte array and returns “yes you have 8 seconds remaining.” 

Martha is preparing a frozen pizza. She pulls the box out of the freezer and looks for instructions. She can see where the instructions are printed, but the text is far too small to read. She pulls out her phone and says “(wake word), Boundri, how do I prepare this pizza?” With the instructions in view of the camera, the system takes a picture and submits a get request to the API query being “how do I prepare this pizza?”, along with the byte array. The API analyzes the query and the byte array and returns the instructions written on the label read out loud to the user. 

## Technologies 

Go: Mux or Echo, GoCV 
Python: Tensorflow, OpenCV, Tesseract 
Swift: iOS, SiriKit
Google Cloud: Compute Instance  

## Upcoming Milestones 

Skateboard (achieved by week 2 out of 7):
Configure an Object Character Recognition (OCR) model that is able to recognize visible text within an image, draw bounding boxes around text regions, and use Tesseract to evaluate those text regions to return a string of the text along with its corresponding region bounding box coordinates. Add OCR route to API and configure the query parameters to pass the image to the OCR model and return JSON of the prediction. Configure Google Cloud instance to host API.  

Bike (achieved week 4 out of 7):
Configure an object detection model that can recognize common objects. Add object detection route to API and configure JSON response of objects and their bounding box coordinates. Add a correlation route that can utilize Go’s concurrency to simultaneously perform a forward pass on one image using both the OCR model and the object detection model, to then compare objects and text regions to calculate the correlation. Ship new routes to Gcloud, and build a basic iOS app that is able to perform a get request on any of the three routes by passing in an image URL.  

Car (achieved week 6 out of 7):
Configure a model that performs data preprocessing to clean up images that have experienced quality corruption due to poor environmental factors including low light, rain, camera quality, etc. Add preprocessing route to API. Increase the functionality of the correlation route to accept a context string, being a question, and use the API’s various models to pull context to answer a basic question involving text and objects. Ship new and updated routes to Gcloud, and update the iOS app to integrate SiriKit functionality for querying the correlation route through Siri. 

Week 7:
We gave ourselves a pillow week to ensure extra time in case of unexpected hiccups. 



